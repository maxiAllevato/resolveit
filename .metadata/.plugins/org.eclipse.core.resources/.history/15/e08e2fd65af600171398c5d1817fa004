package resolveit.exam_resolveit;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.coref.CorefCoreAnnotations.CorefChainAnnotation;
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.CoreAnnotations.LemmaAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PositionAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.StemAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.pipeline.*;
import com.google.gson.Gson;
import com.google.gson.GsonBuilder;



/**
 * Hello world!
 *
 */
public class App 
{
	
	public ArrayList<TokenWord> tokenList = new ArrayList();
	
    public static void main( String[] args )
    {
        System.out.println( "Init Test" );
        
        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty("annotators", "tokenize, ssplit, pos, lemma, ner, parse, dcoref, sentiment");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = "Hi, what is your name. My name is, Maxi";//"For example, fish and fishes could be defined as a unique word by using their stem fish. you buy a book and then you book a room in an hotel.";
        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);
        
     
        
     // these are all the sentences in this document
     // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
     List<CoreMap> sentences = document.get(SentencesAnnotation.class);

     List<TokenWord> listword = new ArrayList<TokenWord>();
     List <Integer> sentList = new ArrayList<Integer>();
     int numSent= 0;
     boolean check = true;
     
     
     
     for(CoreMap sentence: sentences) {
       // traversing the words in the current sentence
       // a CoreLabel is a CoreMap with additional token-specific methods
       for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
         // this is the text of the token
         String word = token.get(TextAnnotation.class);
         // this is the POS tag of the token
         String pos = token.get(PartOfSpeechAnnotation.class);
         // this is the NER label of the token
         String ne = token.get(NamedEntityTagAnnotation.class);
       
         String lem = token.get(LemmaAnnotation.class);
         
         String stem = token.get(StemAnnotation.class);
         
         System.out.println("word " + word + " pos: " + pos + " ne :" + ne+ " lemma: " + lem + " stem: "  + stem);
 
         sentList.add(numSent);
         TokenWord tokenWord =  new TokenWord();
         tokenWord.setLem(lem);
         tokenWord.setPos(pos);
         tokenWord.setWord(word);
         tokenWord.setTotalOccurrences(1);
         //tokenWord.setSentence(sentList);
         //tokenWord.setTotalOccurrences(0);
         
         if(check){
        	 	List <Integer> sentList1 = new ArrayList<Integer>();
        	 
        	 	sentList1.add(numSent);
        	 	tokenWord.setSentence(sentList1);
        	 	listword.add(tokenWord);
        	 	check =false;
        	 
         }else {
        	 	addWord(listword, tokenWord, numSent);
         }
   
  	   
  	   
  	  
      

       }
numSent++;
       // this is the parse tree of the current sentence
       Tree tree = sentence.get(TreeAnnotation.class);

       // this is the Stanford dependency graph of the current sentence
       SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
     }

     // This is the coreference link graph
     // Each chain stores a set of mentions that link to each other,
     // along with a method for getting the most representative mention
     // Both sentence and token offsets start at 1!

     Map<Integer, CorefChain> graph = 
    		  document.get(CorefChainAnnotation.class);
        
     List<ObjectToJson> objList = new ArrayList<ObjectToJson>();
     ObjectToJson obj = new ObjectToJson();
     
     obj.setPos("NN");
     obj.setWord("Hi");
     objList.add(obj);
     
ObjectToJson obj2 = new ObjectToJson();
     
     obj2.setPos("NA");
     obj2.setWord("Good");
     
     objList.add(obj2);
     Gson gson = new Gson();

     //convert java object to JSON format
     String json = gson.toJson(objList);

     Result result = new Result();
     result.setResults(objList);
     
     Gson gson2 = new GsonBuilder().setPrettyPrinting().create();
     String json2 = gson2.toJson(result);
    
     
     Gson gsonPrint = new GsonBuilder().setPrettyPrinting().create();
     String show = gsonPrint.toJson(listword);
     
     System.out.println(show);
     
        
    }
    
    
    public static void addWord(List<TokenWord> listword, TokenWord tokenWord, int numSent) {
    	
    	if(tokenWord.getWord().equals("a") || tokenWord.getWord().equals("the") || tokenWord.getWord().equals("and") 
    			|| tokenWord.getWord().equals("of") || tokenWord.getWord().equals("in") || tokenWord.getWord().equals("be") 
    			|| tokenWord.getWord().equals("also") || tokenWord.getWord().equals("as")) {
    		
    	}else {
    		boolean add = true;
    		Iterator <TokenWord> iterTW = listword.iterator();
    		while(iterTW.hasNext()) {
    			
    			TokenWord tw = iterTW.next();
    			if(tw.getLem().equalsIgnoreCase(tokenWord.getLem())) {
    				int sum = tw.getTotalOccurrences();
    				sum++;
    				tw.setTotalOccurrences(sum);
    				tw.getSentence().add(numSent);
    				add = false;
    				break;
    			}
    			
    			
    			
    		}
    		
    		
    		if(add) {
    			
    		 	 List <Integer> sentList1 = new ArrayList<Integer>();
            	 
            	 sentList1.add(numSent);
            	 tokenWord.setSentence(sentList1);
    			//tokenWord.getSentence().add(numSent);
    			listword.add(tokenWord);
            	
                System.out.println("sorting:");
           	   Collections.sort(listword, tokenWord.StuNameComparator);
    		}
    		
    		/*
    		boolean update = false;
    		for(TokenWord tw : listword) {
    			 if(tw.getLem().equalsIgnoreCase(tokenWord.getLem())) {
   	    		  int sum = tw.getTotalOccurrences();
   	    		  sum ++;
   	    		  tokenWord.setTotalOccurrences(sum);
   	    		  
   	    		  update = true;
   	    		  
   	    		  break;
    			 }

    			
    		}
    		
    		
    		if(update) {
    			listword.get(
    		}else {
    			
    		}
    		  listword.add(tokenWord);
	            	
	                System.out.println("sorting:");
	           	   Collections.sort(listword, tokenWord.StuNameComparator);
	           	 
    		
    			
    		}
    		
    		*/
        	
              
    		
    	}
    		
    		
    
    }   
    	
    
    
}
